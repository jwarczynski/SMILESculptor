{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:08.091825Z",
     "start_time": "2024-12-21T14:25:08.085575Z"
    }
   },
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a585802cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:29.375615Z",
     "start_time": "2024-12-21T14:25:29.339532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "charset_length = 34\n",
    "max_length = 72\n",
    "\n",
    "conv1 = nn.Conv1d(in_channels=charset_length, out_channels=9, kernel_size=9)\n",
    "bn1 = nn.BatchNorm1d(9)\n",
    "\n",
    "conv2 = nn.Conv1d(in_channels=9, out_channels=9, kernel_size=9)\n",
    "bn2 = nn.BatchNorm1d(9)\n",
    "\n",
    "conv3 = nn.Conv1d(in_channels=9, out_channels=10, kernel_size=11)\n",
    "bn3 = nn.BatchNorm1d(10)\n",
    "\n",
    "# Flatten\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "# Dense Layers\n",
    "dense1 = nn.Linear(10 * (max_length - 26), 436)\n",
    "bn_dense1 = nn.BatchNorm1d(436, track_running_stats=False)\n",
    "dropout1 = nn.Dropout(0.083)\n",
    "\n",
    "# Decoder Layers\n",
    "decode_dense1 = nn.Linear(436, 436)\n",
    "decode_bn1 = nn.BatchNorm1d(436, track_running_stats=False)\n",
    "decode_dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "# Repeat Vector (similar to TensorFlow's RepeatVector)\n",
    "max_length = max_length\n",
    "\n",
    "# Recurrent Layer\n",
    "gru_hidden_size = 488\n",
    "gru = nn.GRU(input_size=436, hidden_size=gru_hidden_size, num_layers=3, batch_first=True)\n",
    "\n",
    "# Final layer to reconstruct one-hot encoded sequence\n",
    "reconstruct = nn.Linear(gru_hidden_size, charset_length)"
   ],
   "id": "9fc94ba079dc7395",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:29.944558Z",
     "start_time": "2024-12-21T14:25:29.934726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conv_layers = [\n",
    "    conv1,\n",
    "    bn1,\n",
    "    nn.Tanh(),\n",
    "    conv2,\n",
    "    bn2,\n",
    "    nn.Tanh(),\n",
    "    conv3,\n",
    "    bn3,\n",
    "    nn.Tanh(),\n",
    "]\n",
    "\n",
    "dense_layers = [\n",
    "    dense1,\n",
    "    bn_dense1,\n",
    "    dropout1,\n",
    "    nn.Tanh(),\n",
    "]\n",
    "\n",
    "conv_layers = nn.Sequential(*conv_layers)\n",
    "dense_layers = nn.Sequential(*dense_layers)\n",
    "\n",
    "decoder_dense_layers = [\n",
    "    decode_dense1,\n",
    "    decode_bn1,\n",
    "    decode_dropout1,\n",
    "    nn.Tanh()\n",
    "]\n",
    "\n",
    "decoder_dense_layers = nn.Sequential(*decoder_dense_layers)\n",
    "decor_gru = gru\n",
    "reconstruct_layer = reconstruct\n",
    "\n",
    "def forward_sequential(x):\n",
    "    conv_layers.eval()\n",
    "    dense_layers.eval()\n",
    "    decoder_dense_layers.eval()\n",
    "    gru.eval()\n",
    "    reconstruct_layer.eval()\n",
    "\n",
    "    # Encode\n",
    "    x = conv_layers(x)\n",
    "    x = flatten(x)\n",
    "    x = dense_layers(x)\n",
    "\n",
    "    # Decode\n",
    "    x = decoder_dense_layers(x)\n",
    "    x = x.unsqueeze(1).repeat(1, max_length, 1)\n",
    "    x, _ = gru(x)\n",
    "    x = reconstruct_layer(x)\n",
    "\n",
    "    return x\n",
    "\n"
   ],
   "id": "756b6eff38447465",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:30.620515Z",
     "start_time": "2024-12-21T14:25:30.611788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode(x):\n",
    "    x = conv1(x)\n",
    "    x = bn1(x)\n",
    "    x = torch.tanh(x)\n",
    "\n",
    "    x = conv2(x)\n",
    "    x = bn2(x)\n",
    "    x = torch.tanh(x)\n",
    "\n",
    "    x = conv3(x)\n",
    "    x = bn3(x)\n",
    "    x = torch.tanh(x)\n",
    "\n",
    "    x = flatten(x)\n",
    "\n",
    "    x = dense1(x)\n",
    "    x = bn_dense1(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = dropout1(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def decode(x):\n",
    "    # Decoder Dense Layers\n",
    "    x = decode_dense1(x)\n",
    "    x = decode_bn1(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = decode_dropout1(x)\n",
    "\n",
    "    # Repeat Vector (similar to TensorFlow's RepeatVector)\n",
    "    # Unsqueeze to add sequence dimension and repeat\n",
    "    x = x.unsqueeze(1).repeat(1, max_length, 1)\n",
    "\n",
    "    # GRU Layers\n",
    "    x, _ = gru(x)\n",
    "\n",
    "    # Reconstruct one-hot encoded sequence\n",
    "    x = reconstruct(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def forward_manaul(x):\n",
    "    # set to eval mode every layer\n",
    "    conv1.eval()\n",
    "    bn1.eval()\n",
    "    conv2.eval()\n",
    "    bn2.eval()\n",
    "    conv3.eval()\n",
    "    bn3.eval()\n",
    "    \n",
    "    dense1.eval()\n",
    "    bn_dense1.eval()\n",
    "    dropout1.eval()\n",
    "    \n",
    "    decode_dense1.eval()\n",
    "    decode_bn1.eval()\n",
    "    decode_dropout1.eval()\n",
    "    \n",
    "    gru.eval()\n",
    "    reconstruct.eval()\n",
    "    \n",
    "    # Encode\n",
    "    latent = encode(x)\n",
    "\n",
    "    # Decode\n",
    "    reconstructed = decode(latent)\n",
    "\n",
    "    return reconstructed\n"
   ],
   "id": "3bb49ad66f13bd9f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:31.551184Z",
     "start_time": "2024-12-21T14:25:31.432544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 34, 72)\n",
    "out_seq = forward_sequential(x)\n",
    "out_seq.shape\n",
    "out_man = forward_manaul(x)\n",
    "out_man.shape\n",
    "#comapre if equal\n",
    "torch.allclose(out_seq, out_man)"
   ],
   "id": "dddd1c8817c49ab3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:32.349161Z",
     "start_time": "2024-12-21T14:25:32.344012Z"
    }
   },
   "cell_type": "code",
   "source": "dense_layers",
   "id": "21ae8bca2ee362b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=460, out_features=436, bias=True)\n",
       "  (1): BatchNorm1d(436, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (2): Dropout(p=0.083, inplace=False)\n",
       "  (3): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:33.085267Z",
     "start_time": "2024-12-21T14:25:33.075480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(2, 34, 72)\n",
    "\n",
    "y1 = conv_layers(x)\n",
    "y2 = conv1(x)\n",
    "y2 = bn1(y2)\n",
    "y2 = torch.tanh(y2)\n",
    "y2 = conv2(y2)\n",
    "y2 = bn2(y2)\n",
    "y2 = torch.tanh(y2)\n",
    "y2 = conv3(y2)\n",
    "y2 = bn3(y2)\n",
    "y2 = torch.tanh(y2)\n",
    "\n",
    "torch.allclose(y1, y2)"
   ],
   "id": "f9392adea5e6306f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:33.835631Z",
     "start_time": "2024-12-21T14:25:33.830291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_dense_1 = flatten(y1)\n",
    "y_dense_2 = flatten(y2)\n",
    "torch.allclose(y_dense_1, y_dense_2)"
   ],
   "id": "83fcf29eff84e28f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:34.883895Z",
     "start_time": "2024-12-21T14:25:34.876388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dense1\n",
    "bn_dense1"
   ],
   "id": "8fdd031514cf3634",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(436, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:25:36.195138Z",
     "start_time": "2024-12-21T14:25:36.187029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_dense_1 = dense_layers(y_dense_1)\n",
    "\n",
    "y_dense_2 = dense1(y_dense_2)\n",
    "y_dense_2 = bn_dense1(y_dense_2)\n",
    "y_dense_2 = torch.tanh(y_dense_2)\n",
    "\n",
    "torch.allclose(y_dense_1, y_dense_2)"
   ],
   "id": "2c82a56529484510",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:26:25.561574Z",
     "start_time": "2024-12-21T14:26:25.555689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_decoder_dense_1 = decoder_dense_layers(y_dense_1)\n",
    "y_decoder_dense_2 = decode_dense1(y_dense_2)\n",
    "y_decoder_dense_2 = decode_bn1(y_decoder_dense_2)\n",
    "y_decoder_dense_2 = torch.tanh(y_decoder_dense_2)\n",
    "y_decoder_dense_2 = decode_dropout1(y_decoder_dense_2)\n",
    "\n",
    "torch.allclose(y_decoder_dense_1, y_decoder_dense_2)"
   ],
   "id": "84322410a8f3ed1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:26:43.771162Z",
     "start_time": "2024-12-21T14:26:43.756201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_decoder_dense_1 = y_decoder_dense_1.unsqueeze(1).repeat(1, max_length, 1)\n",
    "y_decoder_dense_2 = y_decoder_dense_2.unsqueeze(1).repeat(1, max_length, 1)\n",
    "\n",
    "torch.allclose(y_decoder_dense_1, y_decoder_dense_2)"
   ],
   "id": "17bff9dd51709d77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:27:19.734052Z",
     "start_time": "2024-12-21T14:27:19.644194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_gru_1, _ = decor_gru(y_decoder_dense_1)\n",
    "y_gru_2, _ = gru(y_decoder_dense_2)\n",
    "\n",
    "torch.allclose(y_gru_1, y_gru_2)"
   ],
   "id": "efaaa84fc1ed16bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T14:27:28.016135Z",
     "start_time": "2024-12-21T14:27:28.009420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_reconstruct_1 = reconstruct_layer(y_gru_1)\n",
    "y_reconstruct_2 = reconstruct(y_gru_2)\n",
    "\n",
    "torch.allclose(y_reconstruct_1, y_reconstruct_2)"
   ],
   "id": "267414d9298580c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:23:18.398683Z",
     "start_time": "2024-12-20T10:23:17.114180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models import MOVVAELightning\n",
    "import yaml\n",
    "\n",
    "with open('../configs/movae_config.yml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "    \n",
    "int_to_char = {i: chr(i + 33) for i in range(34)}\n",
    "\n",
    "model = MOVVAELightning(config['model']['args']['params'], charset_size=34, seq_len=72, loss='bce', lr=0.001, int_to_char=int_to_char)\n",
    "model = model.model\n",
    "\n",
    "x = torch.randn(2, 34, 72)\n",
    "out = model(x)"
   ],
   "id": "988da3e24ed7245f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x192 and 436x436)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m     13\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m34\u001B[39m, \u001B[38;5;241m72\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m out \u001B[38;5;241m=\u001B[39m model(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Side-Projects\\SMILESculptor\\src\\models.py:148\u001B[0m, in \u001B[0;36mMOVAE.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    146\u001B[0m encoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28mprint\u001B[39m(encoded\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m--> 148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(encoded)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\Side-Projects\\SMILESculptor\\src\\models.py:129\u001B[0m, in \u001B[0;36mMOVAEDecoder.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 129\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense_layers(x)\n\u001B[0;32m    130\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mrepeat(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_length, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    131\u001B[0m     x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurrent_layers(x)  \u001B[38;5;66;03m# batch, seq_len, h_dim\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\KE\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (2x192 and 436x436)"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T10:22:56.882575Z",
     "start_time": "2024-12-20T10:22:56.876671Z"
    }
   },
   "cell_type": "code",
   "source": "out.shape",
   "id": "be05b5d30ea55fb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 72, 34])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9402c7540afb8946"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
